{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torchtext.vocab import GloVe\n",
    "from underthesea import word_tokenize\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-26 13:03:41.679031: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1740549821.693559   14920 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1740549821.697894   14920 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2025-02-26 13:03:41.713052: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "PROJECT_ROOT = os.path.expanduser('~/vietnamese-poem-generation')\n",
    "sys.path.append(PROJECT_ROOT)\n",
    "sys.path.insert(0, PROJECT_ROOT)\n",
    "from constants import *\n",
    "sys.path.append(UTILS_DIR)\n",
    "sys.path.append(MODEL_DIR)\n",
    "from tokenization import *\n",
    "from dataset import *\n",
    "from transformer import *\n",
    "from train import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Poem Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>content</th>\n",
       "      <th>title</th>\n",
       "      <th>url</th>\n",
       "      <th>genre</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ngày đông se sắt lạnh trong lòng\\ncó việc đi n...</td>\n",
       "      <td>SAY NẮNG</td>\n",
       "      <td>https://www.facebook.com/groups/48640773509859...</td>\n",
       "      <td>7 chu</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ôm đàn thao thức đến nữa đêm\\nréo rắt cung âm ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>https://www.facebook.com/groups/17645444269765...</td>\n",
       "      <td>7 chu</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>tết có người vui có kẻ buồn\\nngười cười toe to...</td>\n",
       "      <td>TẾT HAI THÁI CỰC</td>\n",
       "      <td>https://www.facebook.com/groups/17645444269765...</td>\n",
       "      <td>7 chu</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>đã quá ba mươi mộng lỡ làng\\nđi tìm day dứt mả...</td>\n",
       "      <td>TRÁI NGANG ĐỨC HẠNH</td>\n",
       "      <td>https://www.facebook.com/groups/48640773509859...</td>\n",
       "      <td>7 chu</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>mai đào nở rộ đón nàng xuân\\nsợi nắng hanh vàn...</td>\n",
       "      <td>DÁNG XUÂN</td>\n",
       "      <td>https://www.facebook.com/groups/17645444269765...</td>\n",
       "      <td>7 chu</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             content                title  \\\n",
       "0  ngày đông se sắt lạnh trong lòng\\ncó việc đi n...             SAY NẮNG   \n",
       "1  ôm đàn thao thức đến nữa đêm\\nréo rắt cung âm ...                  NaN   \n",
       "2  tết có người vui có kẻ buồn\\nngười cười toe to...     TẾT HAI THÁI CỰC   \n",
       "3  đã quá ba mươi mộng lỡ làng\\nđi tìm day dứt mả...  TRÁI NGANG ĐỨC HẠNH   \n",
       "4  mai đào nở rộ đón nàng xuân\\nsợi nắng hanh vàn...            DÁNG XUÂN   \n",
       "\n",
       "                                                 url  genre  \n",
       "0  https://www.facebook.com/groups/48640773509859...  7 chu  \n",
       "1  https://www.facebook.com/groups/17645444269765...  7 chu  \n",
       "2  https://www.facebook.com/groups/17645444269765...  7 chu  \n",
       "3  https://www.facebook.com/groups/48640773509859...  7 chu  \n",
       "4  https://www.facebook.com/groups/17645444269765...  7 chu  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(os.path.join(DATA_DIR, 'poem_dataset.csv'))\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ngày đông se sắt lạnh trong lòng\n",
      "có việc đi ngang qua chỗ đó\n",
      "bắt gặp ánh mắt ai say đắm\n",
      "nụ cười ai khiến tôi hoảng loạn\n",
      "ngày hôm nay tôi lướt qua ai\n",
      "thấy tim mình xốn xang loạn nhịp\n",
      "sao thế có phải mình say nắng\n",
      "chiều đông đâu có nắng mà say\n",
      "say ngất say ngây say không tỉnh\n",
      "lâng lâng như uống phải men tình\n",
      "thả hồn theo gió lộng mênh mông\n",
      "mộng mơ bồng bềnh khi sương xuống\n",
      "mây chiều đã nhuộm đỏ hoàng hôn\n",
      "chim khôn đã cùng nhau về tổ\n",
      "ngất ngây đắm chìm say gì thế\n",
      "mà đứng chôn chân chả muốn về\n",
      "sương nhẹ rơi vương đầy trên cỏ\n",
      "sao lòng buồn bỗng thấy bâng khuâng\n",
      "giật mình chợt tỉnh cơn mộng mị\n",
      "tỉnh lại đi tôi ơi tỉnh lại\n",
      "còn các con đang ngóng chờ cửa\n",
      "còn bạn đời đang đợi tôi về\n"
     ]
    }
   ],
   "source": [
    "print(df['content'][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Dataset for Poem Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Build vocabulary from the text data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = build_vocabulary(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary saved as pkl file\n"
     ]
    }
   ],
   "source": [
    "with open(os.path.join(STORAGE_DIR, 'poem_vocab.pkl'), 'wb') as f:\n",
    "    pickle.dump(vocab, f)\n",
    "print('Vocabulary saved as pkl file')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary loaded from pkl file\n"
     ]
    }
   ],
   "source": [
    "with open(os.path.join(STORAGE_DIR, 'poem_vocab.pkl'), 'rb') as f:\n",
    "    vocab = pickle.load(f)\n",
    "print('Vocabulary loaded from pkl file')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Create poem dataset: In this notebook, I just get about **1000 poems** for training example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = PoemDataset(df=df[:1000], tokenizer=word_tokenize, vocab=vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset saved as pkl file\n"
     ]
    }
   ],
   "source": [
    "with open(os.path.join(STORAGE_DIR, 'poem_dataset.pkl'), 'wb') as f:\n",
    "    pickle.dump(train_dataset, f)\n",
    "print('Dataset saved as pkl file')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset loaded from pkl file\n"
     ]
    }
   ],
   "source": [
    "# load dataset\n",
    "with open(os.path.join(STORAGE_DIR, 'poem_dataset.pkl'), 'rb') as f:\n",
    "    train_dataset = pickle.load(f)\n",
    "print('Dataset loaded from pkl file')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Create data loader:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_dataset, batch_size=4, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "VOCAB_SIZE = len(vocab)\n",
    "EMBEDDING_DIMS = 100\n",
    "HIDDEN_DIMS = 100\n",
    "N_LAYERS = 2\n",
    "N_HEADS = 4\n",
    "DROPOUT = 0.2\n",
    "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "LEARNING_RATE = 0.001\n",
    "N_EPOCHS = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For pre-trained embeddings, I used GloVe with 100-dimensional embeddings:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "glove = GloVe(name=\"6B\", dim=100)\n",
    "pretrained_embedding = glove.vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = TransformerModel(\n",
    "    vocab_size=VOCAB_SIZE,\n",
    "    emb_size=EMBEDDING_DIMS,\n",
    "    num_encoder_layers=N_LAYERS,\n",
    "    nhead=N_HEADS,\n",
    "    dim_feedforward=HIDDEN_DIMS,\n",
    "    dropout=DROPOUT,\n",
    "    device=DEVICE,\n",
    "    pretrained_embedding=pretrained_embedding,\n",
    "    freeze_embedding=True\n",
    ")\n",
    "\n",
    "CRITERION = nn.CrossEntropyLoss()\n",
    "OPTIMIZER = optim.AdamW(model.parameters(), lr=LEARNING_RATE)\n",
    "SCHEDULER = torch.optim.lr_scheduler.StepLR(OPTIMIZER, 1, gamma=0.95)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/100:   5%|▌         | 1167/23307 [00:39<12:29, 29.56it/s, loss=3.0351]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mCRITERION\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mOPTIMIZER\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[43mscheduler\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mSCHEDULER\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mN_EPOCHS\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mDEVICE\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mpoem_transformer\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\n\u001b[1;32m     10\u001b[0m \u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/vietnamese-poem-generation/models/train.py:69\u001b[0m, in \u001b[0;36mtrain_model\u001b[0;34m(model, train_loader, criterion, optimizer, scheduler, num_epochs, device, model_name, sub_batch_size, threshold, patience)\u001b[0m\n\u001b[1;32m     66\u001b[0m     output \u001b[38;5;241m=\u001b[39m output\u001b[38;5;241m.\u001b[39mpermute(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m1\u001b[39m) \u001b[38;5;66;03m# [batch_size, seq_len, vocab_size]\u001b[39;00m\n\u001b[1;32m     67\u001b[0m     loss \u001b[38;5;241m=\u001b[39m criterion(output, mini_tgt_in)\n\u001b[0;32m---> 69\u001b[0m     \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     70\u001b[0m     total_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem()\n\u001b[1;32m     72\u001b[0m torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mclip_grad_norm_(model\u001b[38;5;241m.\u001b[39mparameters(), \u001b[38;5;241m0.5\u001b[39m) \u001b[38;5;66;03m# Clip gradient to prevent exploding\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/_tensor.py:463\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    460\u001b[0m     \u001b[38;5;66;03m# All strings are unicode in Python 3.\u001b[39;00m\n\u001b[1;32m    461\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_tensor_str\u001b[38;5;241m.\u001b[39m_str(\u001b[38;5;28mself\u001b[39m, tensor_contents\u001b[38;5;241m=\u001b[39mtensor_contents)\n\u001b[0;32m--> 463\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mbackward\u001b[39m(\n\u001b[1;32m    464\u001b[0m     \u001b[38;5;28mself\u001b[39m, gradient\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, retain_graph\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, create_graph\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, inputs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    465\u001b[0m ):\n\u001b[1;32m    466\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Computes the gradient of current tensor wrt graph leaves.\u001b[39;00m\n\u001b[1;32m    467\u001b[0m \n\u001b[1;32m    468\u001b[0m \u001b[38;5;124;03m    The graph is differentiated using the chain rule. If the tensor is\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    510\u001b[0m \u001b[38;5;124;03m            used to compute the attr::tensors.\u001b[39;00m\n\u001b[1;32m    511\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m    512\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train_model(\n",
    "    model=model,\n",
    "    train_loader=train_loader,\n",
    "    criterion=CRITERION,\n",
    "    optimizer=OPTIMIZER,\n",
    "    scheduler=SCHEDULER,\n",
    "    num_epochs=N_EPOCHS,\n",
    "    device=DEVICE,\n",
    "    model_name='poem_transformer'\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
